{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ../envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'envs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc3a1a267166>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menvs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_rl_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmake_env\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'envs'"
     ]
    }
   ],
   "source": [
    "from envs import make_rl_envs, make_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(10, 10))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"Mazelab-v2\"\n",
    "# env_name = \"MazelabRandom-v1\"\n",
    "# env_name = \"CartPole-v0\"\n",
    "# env_name = \"cartpole-swingup\"\n",
    "\n",
    "env = make_env(env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = env.reset()\n",
    "print(s.shape)\n",
    "plt.imshow(s.transpose(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array(np.random.rand(1)*100)\n",
    "# a = np.clip(a, env.action_space.low, env.action_space.high)\n",
    "# s, _, done, _ = env.step(env.action_space.sample())\n",
    "s, _, done, _ = env.step(3)\n",
    "plt.imshow(s.transpose(1, 2, 0))\n",
    "\n",
    "if done:\n",
    "    s = env.reset()\n",
    "    plt.imshow(s.transpose(1, 2, 0))\n",
    "    print(done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = make_rl_envs(\n",
    "    env_name,\n",
    "    0,\n",
    "    2,\n",
    "    None,\n",
    "    frame_stack=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = envs.reset()\n",
    "plt.imshow(s[1].numpy().transpose(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a2c_ppo_acktr import algo, utils\n",
    "from a2c_ppo_acktr.algo import gail\n",
    "from a2c_ppo_acktr.arguments import get_args\n",
    "from a2c_ppo_acktr.envs import make_vec_envs\n",
    "from a2c_ppo_acktr.model import Policy\n",
    "from a2c_ppo_acktr.storage import RolloutStorage\n",
    "from evaluation import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "seed = int(time.time()*1000)\n",
    "print(seed)\n",
    "cuda = True\n",
    "log_dir = \"log\"\n",
    "# env_name = \"Mazelab-v1\"\n",
    "env_name = \"Mazelab-v2\"\n",
    "# env_name = \"Mazelab-v3\"\n",
    "# env_name = \"Mazelab-v4\"\n",
    "# env_name = \"Mazelab-v5\"\n",
    "\n",
    "num_envs = 32\n",
    "recurrent_policy = False\n",
    "algo_type = \"ppo\"\n",
    "\n",
    "value_loss_coef = 0.5\n",
    "lr = 2.5e-4\n",
    "entropy_coef = 0.01\n",
    "eps = 1e-5\n",
    "alpha = 0.99\n",
    "max_grad_norm = 0.5\n",
    "clip_param = 0.2\n",
    "ppo_epoch = 4\n",
    "num_mini_batch = 32\n",
    "gamma = 0.99\n",
    "num_steps = 128\n",
    "num_env_steps = 1e7\n",
    "use_linear_lr_decay = True\n",
    "\n",
    "use_gae = False\n",
    "gae_lambda = 0.95\n",
    "use_proper_time_limits = False\n",
    "save_interval = 100\n",
    "save_dir = \"save\"\n",
    "log_interval = 5\n",
    "eval_interval = 10\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "if cuda and torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "log_dir = os.path.expanduser(log_dir)\n",
    "eval_log_dir = log_dir + \"_eval\"\n",
    "utils.cleanup_log_dir(log_dir)\n",
    "utils.cleanup_log_dir(eval_log_dir)\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "device = torch.device(\"cuda:0\" if cuda else \"cpu\")\n",
    "\n",
    "\n",
    "envs = make_rl_envs(\n",
    "    env_name,\n",
    "    seed,\n",
    "    num_envs,\n",
    "    device,\n",
    "    frame_stack=1\n",
    ")\n",
    "\n",
    "\n",
    "actor_critic = Policy(\n",
    "    envs.observation_space.shape,\n",
    "    envs.action_space,\n",
    "    base_kwargs={'recurrent': recurrent_policy})\n",
    "actor_critic.to(device)\n",
    "\n",
    "\n",
    "\n",
    "if algo_type == 'a2c':\n",
    "    agent = algo.A2C_ACKTR(\n",
    "        actor_critic,\n",
    "        value_loss_coef,\n",
    "        entropy_coef,\n",
    "        lr=lr,\n",
    "        eps=eps,\n",
    "        alpha=alpha,\n",
    "        max_grad_norm=max_grad_norm)\n",
    "elif algo_type == 'ppo':\n",
    "    agent = algo.PPO(\n",
    "        actor_critic,\n",
    "        clip_param,\n",
    "        ppo_epoch,\n",
    "        num_mini_batch,\n",
    "        value_loss_coef,\n",
    "        entropy_coef,\n",
    "        lr=lr,\n",
    "        eps=eps,\n",
    "        max_grad_norm=max_grad_norm)\n",
    "elif algo_type == 'acktr':\n",
    "    agent = algo.A2C_ACKTR(\n",
    "        actor_critic, value_loss_coef, entropy_coef, acktr=True)\n",
    "\n",
    "\n",
    "rollouts = RolloutStorage(num_steps, num_envs,\n",
    "                          envs.observation_space.shape, envs.action_space,\n",
    "                          actor_critic.recurrent_hidden_state_size)\n",
    "\n",
    "obs = envs.reset()\n",
    "rollouts.obs[0].copy_(obs)\n",
    "rollouts.to(device)\n",
    "\n",
    "episode_rewards = deque(maxlen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actor_critic, env_name, seed, num_processes, eval_log_dir,\n",
    "             device):\n",
    "    eval_envs = make_rl_envs(env_name, seed + num_processes, num_processes,\n",
    "                              device, frame_stack=1)\n",
    "    eval_episode_rewards = deque(maxlen=30)\n",
    "\n",
    "    obs = eval_envs.reset()\n",
    "    eval_recurrent_hidden_states = torch.zeros(\n",
    "        num_processes, actor_critic.recurrent_hidden_state_size, device=device)\n",
    "    eval_masks = torch.zeros(num_processes, 1, device=device)\n",
    "\n",
    "    while len(eval_episode_rewards) < 30:\n",
    "        with torch.no_grad():\n",
    "            _, action, _, eval_recurrent_hidden_states = actor_critic.act(\n",
    "                obs,\n",
    "                eval_recurrent_hidden_states,\n",
    "                eval_masks,\n",
    "                deterministic=True)\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, _, done, infos = eval_envs.step(action)\n",
    "\n",
    "        eval_masks = torch.tensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done],\n",
    "            dtype=torch.float32,\n",
    "            device=device)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                eval_episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "    eval_envs.close()\n",
    "    return eval_episode_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs[0].cpu().numpy().transpose(1, 2, 0)[:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "num_updates = int(num_env_steps) // num_steps // num_envs\n",
    "for j in range(num_updates):\n",
    "\n",
    "    if use_linear_lr_decay:\n",
    "        # decrease learning rate linearly\n",
    "        utils.update_linear_schedule(\n",
    "            agent.optimizer, j, num_updates,\n",
    "            agent.optimizer.lr if algo_type == \"acktr\" else lr)\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        # Sample actions\n",
    "        with torch.no_grad():\n",
    "            value, action, action_log_prob, recurrent_hidden_states = actor_critic.act(\n",
    "                rollouts.obs[step], rollouts.recurrent_hidden_states[step],\n",
    "                rollouts.masks[step])\n",
    "\n",
    "        # Obser reward and next obs\n",
    "        obs, reward, done, infos = envs.step(action)\n",
    "\n",
    "        for info in infos:\n",
    "            if 'episode' in info.keys():\n",
    "                episode_rewards.append(info['episode']['r'])\n",
    "\n",
    "        # If done then clean the history of observations.\n",
    "        masks = torch.FloatTensor(\n",
    "            [[0.0] if done_ else [1.0] for done_ in done])\n",
    "        bad_masks = torch.FloatTensor(\n",
    "            [[0.0] if 'bad_transition' in info.keys() else [1.0]\n",
    "             for info in infos])\n",
    "        rollouts.insert(obs, recurrent_hidden_states, action,\n",
    "                        action_log_prob, value, reward, masks, bad_masks)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        next_value = actor_critic.get_value(\n",
    "            rollouts.obs[-1], rollouts.recurrent_hidden_states[-1],\n",
    "            rollouts.masks[-1]).detach()\n",
    "\n",
    "\n",
    "\n",
    "    rollouts.compute_returns(next_value, use_gae, gamma,\n",
    "                             gae_lambda, use_proper_time_limits)\n",
    "\n",
    "    value_loss, action_loss, dist_entropy = agent.update(rollouts)\n",
    "\n",
    "    rollouts.after_update()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # save for every interval-th episode or for the last epoch\n",
    "    if (j % save_interval == 0\n",
    "            or j == num_updates - 1) and save_dir != \"\":\n",
    "        save_path = os.path.join(save_dir, algo_type)\n",
    "        try:\n",
    "            os.makedirs(save_path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "        torch.save([\n",
    "            actor_critic,\n",
    "            getattr(utils.get_vec_normalize(envs), 'ob_rms', None)\n",
    "        ], os.path.join(save_path, env_name + \".pt\"))\n",
    "\n",
    "    if j % log_interval == 0 and len(episode_rewards) > 1:\n",
    "        total_num_steps = (j + 1) * num_envs * num_steps\n",
    "        end = time.time()\n",
    "        print(\n",
    "            \"\"\"Updates {}, num timesteps {}, FPS {}\n",
    "            Last {} training episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\n",
    "            entropy: {:.2f} value loss: {:.3f} action loss {:.3f}\n",
    "            \"\"\".format(\n",
    "                j, total_num_steps,\n",
    "                int(total_num_steps / (end - start)),\n",
    "                len(episode_rewards), np.mean(episode_rewards),\n",
    "                np.median(episode_rewards), np.min(episode_rewards),\n",
    "                np.max(episode_rewards),\n",
    "                dist_entropy, value_loss, action_loss)\n",
    "        )\n",
    "\n",
    "    if (eval_interval is not None and len(episode_rewards) > 1\n",
    "            and j % eval_interval == 0):\n",
    "        eval_episode_rewards = evaluate(\n",
    "            actor_critic, env_name, seed,\n",
    "            num_envs, eval_log_dir, device)\n",
    "        print(\n",
    "            \"{} eval episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}\\n\"\n",
    "            .format(\n",
    "                len(eval_episode_rewards), np.mean(eval_episode_rewards),\n",
    "                np.median(eval_episode_rewards), np.min(eval_episode_rewards),\n",
    "                np.max(eval_episode_rewards))\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts.obs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_seq = rollouts.obs[:, 0, 2, :, :].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(obs_seq[100:109,:,:].reshape(-1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
